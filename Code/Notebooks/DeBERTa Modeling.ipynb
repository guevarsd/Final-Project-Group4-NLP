{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba5f850",
   "metadata": {},
   "source": [
    "# DeBERTa\n",
    "### Microsoft's Decoding-enhanced BERT with Disentangled Attention\n",
    "\n",
    "Hugging Face: https://huggingface.co/microsoft/deberta-base  \n",
    "Models: https://huggingface.co/models?sort=downloads&search=debert  \n",
    "Paper : https://arxiv.org/abs/2006.03654  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "372f383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "#!pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29b468dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncomment if download needed\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# This section is the initial downloads of all the GLUE Datasets\n",
    "# This only needs to be run once, but if you run it again\n",
    "# it will check the directory in EC2 before it downloads again.\n",
    "#\n",
    "\n",
    "from datasets import load_dataset\n",
    "\"\"\"\n",
    "Reference for the GLUE datasets:\n",
    "https://huggingface.co/datasets/glue\n",
    "\"\"\"\n",
    "\n",
    "glue_datasets = ['cola', 'mnli', 'mrpc','qnli', 'qqp', 'rte', 'sst2', 'stsb', 'wnli']\n",
    "\n",
    "\"\"\"\n",
    "#Download all the datasets for initial set-up\n",
    "for challenge in glue_datasets:\n",
    "    print('\\n\\n',challenge)\n",
    "    dataset_train = load_dataset('glue', challenge, split='train')\n",
    "    try:\n",
    "        dataset_test = load_dataset('glue', challenge, split='test')\n",
    "    except:\n",
    "        dataset_test = load_dataset('glue', challenge, split='test_matched')\n",
    "        dataset_test = load_dataset('glue', challenge, split='test_mismatched')\n",
    "\"\"\"\n",
    "print('Uncomment if download needed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6f93bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====\\/  cola  \\/====\n",
      "\n",
      "Dataset({\n",
      "    features: ['sentence', 'label', 'idx'],\n",
      "    num_rows: 8551\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "=====\\/  mnli  \\/====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "    num_rows: 392702\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "=====\\/  mrpc  \\/====\n",
      "\n",
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "    num_rows: 3668\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "=====\\/  qnli  \\/====\n",
      "\n",
      "Dataset({\n",
      "    features: ['question', 'sentence', 'label', 'idx'],\n",
      "    num_rows: 104743\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "=====\\/  qqp  \\/====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question1', 'question2', 'label', 'idx'],\n",
      "    num_rows: 363846\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "=====\\/  rte  \\/====\n",
      "\n",
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "    num_rows: 2490\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "=====\\/  sst2  \\/====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'label', 'idx'],\n",
      "    num_rows: 67349\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "=====\\/  stsb  \\/====\n",
      "\n",
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "    num_rows: 5749\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "=====\\/  wnli  \\/====\n",
      "\n",
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "    num_rows: 635\n",
      "})\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_options = []\n",
    "\n",
    "for dset in glue_datasets:\n",
    "    print(f'=====\\/  {dset}  \\/====\\n')\n",
    "    raw_datasets = load_dataset(\"glue\", dset)\n",
    "    feature_options.append(list(raw_datasets['train'].features.keys()))\n",
    "    print(raw_datasets['train'])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "363c7e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sentence', 'label', 'idx'],\n",
       " ['premise', 'hypothesis', 'label', 'idx'],\n",
       " ['sentence1', 'sentence2', 'label', 'idx'],\n",
       " ['question', 'sentence', 'label', 'idx'],\n",
       " ['question1', 'question2', 'label', 'idx'],\n",
       " ['sentence1', 'sentence2', 'label', 'idx'],\n",
       " ['sentence', 'label', 'idx'],\n",
       " ['sentence1', 'sentence2', 'label', 'idx'],\n",
       " ['sentence1', 'sentence2', 'label', 'idx']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b76b2239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5.       , 3.8      , 2.6      , 4.25     , 0.5      , 1.6      ,\n",
       "       2.2      , 4.2      , 4.6      , 3.867    , 4.667    , 1.667    ,\n",
       "       3.75     , 3.2      , 2.8      , 3.       , 4.8      , 4.       ,\n",
       "       4.909    , 2.4      , 3.4      , 2.75     , 3.6      , 1.75     ,\n",
       "       1.       , 2.375    , 4.4      , 4.75     , 1.556    , 3.938    ,\n",
       "       3.5      , 1.4      , 3.833    , 0.6      , 2.917    , 2.       ,\n",
       "       0.8      , 1.643    , 2.25     , 4.857    , 2.533    , 0.143    ,\n",
       "       2.5      , 0.       , 0.4      , 0.667    , 4.133    , 1.2      ,\n",
       "       3.765    , 3.941    , 0.25     , 3.25     , 0.75     , 1.5      ,\n",
       "       0.2      , 3.111    , 1.286    , 1.8      , 0.85     , 3.923    ,\n",
       "       1.25     , 0.833    , 0.333    , 3.333    , 4.333    , 2.667    ,\n",
       "       0.417    , 2.818    , 3.533    , 0.643    , 1.583    , 1.778    ,\n",
       "       3.667    , 2.333    , 1.7      , 4.5      , 0.727    , 1.333    ,\n",
       "       0.067    , 4.875    , 3.615    , 2.875    , 4.091    , 2.769    ,\n",
       "       2.583    , 3.929    , 0.231    , 0.118    , 4.1      , 2.33     ,\n",
       "       0.17     , 3.67     , 2.83     , 3.1      , 2.1111112, 1.1      ,\n",
       "       3.7777777, 4.571429 , 2.4666667, 0.9      , 1.5333333, 3.846    ,\n",
       "       3.875    , 1.846    , 2.647    , 3.067    , 4.778    , 4.364    ,\n",
       "       3.786    , 4.923    , 4.571    , 3.167    , 0.944    , 3.056    ,\n",
       "       4.818    , 3.231    , 4.727    , 1.733    , 2.909    , 3.438    ,\n",
       "       4.176    , 2.625    , 3.455    , 4.056    , 3.643    , 3.692    ,\n",
       "       3.857    , 1.273    , 2.588    , 3.444    , 0.889    , 3.273    ,\n",
       "       2.7      , 3.909    , 3.933    , 3.769    , 3.625    , 4.308    ,\n",
       "       3.3333333, 4.33     ], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = \"stsb\"\n",
    "load_dataset('glue', task, split='train').to_pandas().label.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3022a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "#Start with Cola\n",
    "\n",
    "challenge = 'cola'\n",
    "\n",
    "train_data = load_dataset('glue', challenge, split='train')\n",
    "test_data = load_dataset('glue', challenge, split='test')\n",
    "valid_data = load_dataset('glue', challenge, split='validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fcf752a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-7631f19924bf8e90.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94703f2f6bc448dc82cb301d9760f04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f818b1239ef0a01e.arrow\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.classifer.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifer.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6947, grad_fn=<NllLossBackward>) torch.Size([8, 2])\n",
      "400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2fb7bc4f854c6e83830242af0d745e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.36619718309859156}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###  Imports  ###\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "###  Set-up  ###\n",
    "num_epochs = 5\n",
    "checkpoint = \"microsoft/deberta-v3-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "task = \"wnli\"\n",
    "\n",
    "###  Choose Task Dataset  ###\n",
    "raw_datasets = load_dataset(\"glue\", task)\n",
    "\n",
    "\n",
    "###  Tokenize  ###\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names\n",
    "\n",
    "\n",
    "###  Dataloader  ###\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"],\n",
    "                              shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"],\n",
    "                             batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}\n",
    "\n",
    "\n",
    "###  Model Set-up  ###\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler( \"linear\", optimizer=optimizer,\n",
    "                              num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "print(num_training_steps)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device\n",
    "\n",
    "#Tqdm\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "###  Evaluations  ###\n",
    "metric = load_metric(\"glue\", task)\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ba1961b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'sentence1' in raw_datasets['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1dfa88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
