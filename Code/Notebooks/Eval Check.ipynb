{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a222958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric\n",
    "import transformers\n",
    "import os\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee9e31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of glue tasks\n",
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]\n",
    "\n",
    "#List of glue keys\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7aca972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "###  Tokenizing Section  ####\n",
    "\n",
    "#Load model\n",
    "model_checkpoint = \"microsoft/deberta-v3-small\"\n",
    "\n",
    "# Create tokenizer for respective model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True, truncation=True, model_max_length=512)\n",
    "\n",
    "def tokenizer_func(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True,)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9072282d",
   "metadata": {},
   "source": [
    "## Load Deberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a3ac3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd3b9fe5c8c48a6b09e3a74d8927e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16802b2fd7634e4eb99f5a87eb73ff06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b09f14fc8649cf8f2688a1e8cf5cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.classifer.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifer.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stsb: {'eval_loss': 7.932581901550293, 'eval_pearson': 0.05494078201004924, 'eval_spearmanr': 0.06668472923458942, 'eval_runtime': 4.7819, 'eval_samples_per_second': 313.682, 'eval_steps_per_second': 31.368}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collector=[]\n",
    "\n",
    "for task in GLUE_TASKS:\n",
    "\n",
    "    #Select task\n",
    "    batch_size = 10 #10 normally\n",
    "\n",
    "    # Load dataset based on task variable\n",
    "    actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "    dataset = load_dataset(\"glue\", actual_task)\n",
    "    metric = load_metric('glue', actual_task)\n",
    "\n",
    "\n",
    "\n",
    "    #Collect sentence keys and labels\n",
    "    sentence1_key, sentence2_key = task_to_keys[task]\n",
    "\n",
    "    # tokenize sentence(s)\n",
    "    encoded_dataset = dataset.map(tokenizer_func, batched=True)\n",
    "\n",
    "\n",
    "    # Number of logits to output\n",
    "    num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
    "\n",
    "    #Insert the model checkpoint you want to test\n",
    "    model_checkpoint = \"microsoft/deberta-v3-small\"\n",
    "    #model_checkpoint = \"deberta-v3-small_baseline_\"+actual_task+\"/\"\n",
    "    #model_checkpoint = \"deberta-v3-small_tuned_\"+actual_task+\"/\"\n",
    "    \n",
    "    ###  Model Section  ####\n",
    "\n",
    "    # Create model and attach ForSequenceClassification head\n",
    "    model_deberta = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "\n",
    "    # Type of metric for given task\n",
    "    metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        f\"{model_checkpoint}-finetuned-Testing-{task}\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=0.01,\n",
    "        metric_for_best_model=metric_name,\n",
    "        eval_accumulation_steps=5\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        if task != \"stsb\":\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "        else:\n",
    "            predictions = predictions[:]#, 0]\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "    trainer = Trainer(\n",
    "        model_deberta,\n",
    "        args,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[validation_key],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    score = trainer.evaluate()\n",
    "    print(f'{actual_task}: {score}\\n\\n')\n",
    "    collector.append([actual_task, metric_name, score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af7e6879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['stsb',\n",
       "  'pearson',\n",
       "  {'eval_loss': 7.932581901550293,\n",
       "   'eval_pearson': 0.05494078201004924,\n",
       "   'eval_spearmanr': 0.06668472923458942,\n",
       "   'eval_runtime': 4.7819,\n",
       "   'eval_samples_per_second': 313.682,\n",
       "   'eval_steps_per_second': 31.368}]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099868b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
