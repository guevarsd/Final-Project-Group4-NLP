{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37784929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric\n",
    "import transformers\n",
    "import os\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "metric_collector = []\n",
    "\n",
    "\n",
    "# List of glue tasks\n",
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d0447d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /home/ubuntu/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n",
      "Model config DebertaV2Config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/spm.model from cache at /home/ubuntu/.cache/huggingface/transformers/3ed0740946d0a60434dd6a0c940068899c0b48bb5caba7d60c1db454877c64a3.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/transformers/b40830d1301d39fdc8c6a059787f7f46b8786c252b5475512aa5cf0a66020075.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-small/resolve/main/config.json from cache at /home/ubuntu/.cache/huggingface/transformers/8e0c12a7672d1d36f647c86e5fc3a911f189d8704e2bc94dde4a1ffe38f648fa.9df96bac06c2c492bc77ad040068f903c93beec14607428f25bf9081644ad0da\n",
      "Model config DebertaV2Config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Adding [MASK] to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730ec6bccc50412ab8a48899d7fd2bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430a70d03b504d7699e0828f18d0e29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3941f0baa25b44c4bd9ac88488e6c086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file deberta-v3-small_baseline_stsb/config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-small\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaV2ForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file deberta-v3-small_baseline_stsb/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DebertaV2ForSequenceClassification.\n",
      "\n",
      "All the weights of DebertaV2ForSequenceClassification were initialized from the model checkpoint at deberta-v3-small_baseline_stsb/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 10\n",
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ubuntu/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/google/electra-small-discriminator/resolve/main/vocab.txt from cache at /home/ubuntu/.cache/huggingface/transformers/ece45ade3e01224cf31fed8e183b306d17b84e8abd415363474cfe72274f7814.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/google/electra-small-discriminator/resolve/main/tokenizer.json from cache at /home/ubuntu/.cache/huggingface/transformers/92992b36de47dee64b1d5a31c05d8d51e3075b918a218f5ba4f6e306c4b81b8c.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/google/electra-small-discriminator/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/google/electra-small-discriminator/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/google/electra-small-discriminator/resolve/main/tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/transformers/8b3aea9f7242b3d19268df5b1bfed8f66e08671a72ac0809ada08e5ef1adc592.19eda9a6da5fb0e52a45200c95876729561dde16a69b9116953af6edca1d1e92\n",
      "loading configuration file https://huggingface.co/google/electra-small-discriminator/resolve/main/config.json from cache at /home/ubuntu/.cache/huggingface/transformers/ca13c16218c6780ec76753d3afa19fcb7cc759e3f63ee87e441562d374762b3d.3dd1921e571dfa18c0bdaa17b9b38f111097812281989b1cb22263738e66ef73\n",
      "Model config ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3724e2f4e6724fc9820e310f85e0efdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72a98a8724f4b4f82ba70105c5b49b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acb9bb93a8449fa8ce931bde9582528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file Electra_fintuned_stsb/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file Electra_fintuned_stsb/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraForSequenceClassification.\n",
      "\n",
      "All the weights of ElectraForSequenceClassification were initialized from the model checkpoint at Electra_fintuned_stsb/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `ElectraForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 10\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /home/ubuntu/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
      "Model config XLNetConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLNetLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"untie_r\": true,\n",
      "  \"use_mems_eval\": true,\n",
      "  \"use_mems_train\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlnet-base-cased/resolve/main/spiece.model from cache at /home/ubuntu/.cache/huggingface/transformers/df73bc9f8d13bf2ea4dab95624895e45a550a0f0a825e41fc25440bf367ee3c8.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9\n",
      "loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer.json from cache at /home/ubuntu/.cache/huggingface/transformers/46f47734f3dcaef7e236b9a3e887f27814e18836a8db7e6a49148000058a1a54.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53\n",
      "loading file https://huggingface.co/xlnet-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlnet-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlnet-base-cased/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/xlnet-base-cased/resolve/main/config.json from cache at /home/ubuntu/.cache/huggingface/transformers/06bdb0f5882dbb833618c81c3b4c996a0c79422fa2c95ffea3827f92fc2dba6b.da982e2e596ec73828dbae86525a1870e513bd63aae5a2dc773ccc840ac5c346\n",
      "Model config XLNetConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLNetLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"untie_r\": true,\n",
      "  \"use_mems_eval\": true,\n",
      "  \"use_mems_train\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2cbb97c2a247538bf13aae14def280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1f9bc59caa4a7c90a05e90e981058e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d8fe96fe2c4593891befc701b4fd15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file xlnet-base-cased_baseline_stsb/config.json\n",
      "Model config XLNetConfig {\n",
      "  \"_name_or_path\": \"xlnet-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"XLNetForSequenceClassification\"\n",
      "  ],\n",
      "  \"attn_type\": \"bi\",\n",
      "  \"bi_data\": false,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"clamp_len\": -1,\n",
      "  \"d_head\": 64,\n",
      "  \"d_inner\": 3072,\n",
      "  \"d_model\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"end_n_top\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_activation\": \"gelu\",\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mem_len\": null,\n",
      "  \"model_type\": \"xlnet\",\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"pad_token_id\": 5,\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"reuse_len\": null,\n",
      "  \"same_length\": false,\n",
      "  \"start_n_top\": 5,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"last\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 250\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"untie_r\": true,\n",
      "  \"use_mems_eval\": true,\n",
      "  \"use_mems_train\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file xlnet-base-cased_baseline_stsb/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLNetForSequenceClassification.\n",
      "\n",
      "All the weights of XLNetForSequenceClassification were initialized from the model checkpoint at xlnet-base-cased_baseline_stsb/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLNetForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1500\n",
      "  Batch size = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   deberta_0\n",
      "0   4.966459\n",
      "1   4.890520\n",
      "2   5.022237\n",
      "3   3.293687\n",
      "4   3.372666 \n",
      "\n",
      "   electra_0\n",
      "0   4.428692\n",
      "1   4.328940\n",
      "2   4.411211\n",
      "3   2.389011\n",
      "4   2.390966 \n",
      "\n",
      "    xlnet_0\n",
      "0  5.197846\n",
      "1  5.004109\n",
      "2  5.168311\n",
      "3  2.254660\n",
      "4  2.450130 \n",
      "\n",
      "\n",
      "\n",
      "TASK :  stsb\n",
      "Results Using All Features: \n",
      "\n",
      "Accuracy :  89.8923116959467 \n",
      "Fscore :  0.892027894308892\n",
      "-------------------\n",
      "DeBERTa :  86.7889875669144\n",
      "Electra :  87.2716536910814\n",
      "XLNet :  89.26391576772032\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for task in GLUE_TASKS:\n",
    "    \n",
    "    #List of glue keys\n",
    "    task_to_keys = {\n",
    "        \"cola\": (\"sentence\", None),\n",
    "        \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "        \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "        \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "        \"qnli\": (\"question\", \"sentence\"),\n",
    "        \"qqp\": (\"question1\", \"question2\"),\n",
    "        \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "        \"sst2\": (\"sentence\", None),\n",
    "        \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "        \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "    }\n",
    "    \n",
    "    #Select task\n",
    "    #task = \"rte\"  #cola, mrpc\n",
    "    batch_size = 10 #10 normally, 8 for qnli\n",
    "    \n",
    "    # Load dataset based on task variable\n",
    "    actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "    dataset = load_dataset(\"glue\", actual_task)\n",
    "    metric = load_metric('glue', actual_task)\n",
    "    \n",
    "    #Collect sentence keys and labels\n",
    "    sentence1_key, sentence2_key = task_to_keys[task]\n",
    "    \n",
    "    # Number of logits to output\n",
    "    num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
    "    \n",
    "    \n",
    "\n",
    "    ###############################################\n",
    "    \n",
    "    #         DEBERTA SECTION\n",
    "    \n",
    "    ###############################################\n",
    "    \n",
    "    \n",
    "    ###  Tokenizing Section  ####\n",
    "    \n",
    "    #Load model\n",
    "    model_checkpoint = \"microsoft/deberta-v3-small\"\n",
    "    \n",
    "    # Create tokenizer for respective model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True, truncation=True, model_max_length=512)\n",
    "    \n",
    "    def tokenizer_func(examples):\n",
    "        if sentence2_key is None:\n",
    "            return tokenizer(examples[sentence1_key], truncation=True,)\n",
    "        return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True,)\n",
    "    \n",
    "    # tokenize sentence(s)\n",
    "    encoded_dataset = dataset.map(tokenizer_func, batched=True)\n",
    "    \n",
    "    #model_checkpoint = \"deberta-v3-small_baseline_cola/\"\n",
    "    model_checkpoint = \"deberta-v3-small_baseline_\"+actual_task+\"/\"\n",
    "    \n",
    "    ###  Model Section  ####\n",
    "    \n",
    "    # Create model and attach ForSequenceClassification head\n",
    "    model_deberta = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "    \n",
    "    # Type of metric for given task\n",
    "    metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        f\"{model_checkpoint}-finetuned-Testing-{task}\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=0.01,\n",
    "        metric_for_best_model=metric_name,\n",
    "        eval_accumulation_steps=5\n",
    "    )\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        if task != \"stsb\":\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "        else:\n",
    "            predictions = predictions[:]#, 0]\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "    trainer = Trainer(\n",
    "        model_deberta,\n",
    "        args,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[validation_key],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    trainer.evaluate()\n",
    "    \n",
    "\n",
    "    \n",
    "    ### Collect Predictions  ###\n",
    "    \n",
    "    prediction_deberta = trainer.predict(encoded_dataset[validation_key])\n",
    "    \n",
    "\n",
    "    \n",
    "    ## Clear the Cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "    ###############################################\n",
    "    \n",
    "    #         ELECTRA SECTION\n",
    "    \n",
    "    ###############################################\n",
    "    \n",
    "    \n",
    "    ###  Tokenizing Section  ####\n",
    "    \n",
    "    #Load model\n",
    "    model_checkpoint = \"google/electra-small-discriminator\"\n",
    "    \n",
    "    # Create tokenizer for respective model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True, truncation=True, model_max_length=512)\n",
    "    \n",
    "    def tokenizer_func(examples):\n",
    "        if sentence2_key is None:\n",
    "            return tokenizer(examples[sentence1_key], truncation=True,)\n",
    "        return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True,)\n",
    "    \n",
    "    # tokenize sentence(s)\n",
    "    encoded_dataset = dataset.map(tokenizer_func, batched=True)\n",
    "    \n",
    "    #model_checkpoint = \"electra-small-discriminator-finetuned-cola/\"\n",
    "    #model_checkpoint = \"Electra_fintuned_cola/\"\n",
    "    model_checkpoint = \"Electra_fintuned_\"+actual_task+\"/\"\n",
    "    \n",
    "    ###  Model Section  ####\n",
    "    \n",
    "    # Create model and attach ForSequenceClassification head\n",
    "    model_electra = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "    \n",
    "    # Type of metric for given task\n",
    "    metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "    \n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        f\"{model_checkpoint}-finetuned-Testing-{task}\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=0.01,\n",
    "        metric_for_best_model=metric_name,\n",
    "        eval_accumulation_steps=5\n",
    "    )\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        if task != \"stsb\":\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "        else:\n",
    "            predictions = predictions[:, 0]\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "    trainer = Trainer(\n",
    "        model_electra,\n",
    "        args,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[validation_key],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    trainer.evaluate()\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    ### Collect Predictions  ###\n",
    "    ## Clear the Cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    prediction_electra = trainer.predict(encoded_dataset[validation_key])\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    ## Clear the Cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "\n",
    "    ###############################################\n",
    "    \n",
    "    #         XLNET SECTION\n",
    "    \n",
    "    ###############################################\n",
    "    \n",
    "    \n",
    "    ###  Tokenizing Section  ####\n",
    "    \n",
    "    #Load model\n",
    "    model_checkpoint = \"xlnet-base-cased\"\n",
    "    \n",
    "    # Create tokenizer for respective model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True, truncation=True, model_max_length=512)\n",
    "    \n",
    "    def tokenizer_func(examples):\n",
    "        if sentence2_key is None:\n",
    "            return tokenizer(examples[sentence1_key], truncation=True,)\n",
    "        return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True,)\n",
    "    \n",
    "    # tokenize sentence(s)\n",
    "    encoded_dataset = dataset.map(tokenizer_func, batched=True)\n",
    "    \n",
    "    #model_checkpoint = \"electra-small-discriminator-finetuned-cola/\"\n",
    "    #model_checkpoint = \"Electra_fintuned_cola/\"\n",
    "    model_checkpoint = \"xlnet-base-cased_baseline_\"+actual_task+\"/\"\n",
    "    \n",
    "    ###  Model Section  ####\n",
    "    \n",
    "    # Create model and attach ForSequenceClassification head\n",
    "    model_xlnet = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "    \n",
    "    # Type of metric for given task\n",
    "    metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "    \n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        f\"{model_checkpoint}-finetuned-Testing-{task}\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        weight_decay=0.01,\n",
    "        metric_for_best_model=metric_name,\n",
    "        eval_accumulation_steps=5\n",
    "    )\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        if task != \"stsb\":\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "        else:\n",
    "            predictions = predictions[:, 0]\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "    trainer = Trainer(\n",
    "        model_xlnet,\n",
    "        args,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[validation_key],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    trainer.evaluate()\n",
    "    \n",
    "\n",
    "    \n",
    "    ### Collect Predictions  ###\n",
    "    ## Clear the Cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    prediction_xlnet = trainer.predict(encoded_dataset[validation_key])\n",
    "    \n",
    "\n",
    "    \n",
    "    ## Clear the Cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    ###############################################\n",
    "    \n",
    "    # Combine Model Predicions to create Input Features\n",
    "    \n",
    "    ###############################################\n",
    "    \n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    #Labels\n",
    "    val_labels = prediction_deberta.label_ids\n",
    "    \n",
    "    \n",
    "    #DeBERTa\n",
    "    df_deberta = pd.DataFrame(prediction_deberta[0])\n",
    "    df_deberta=df_deberta.rename(columns=dict(zip(df_deberta.columns,['deberta_'+str(col) for col in df_deberta.columns])))\n",
    "    print(df_deberta.head(),'\\n')\n",
    "    \n",
    "    \n",
    "    #Electra\n",
    "    df_electra = pd.DataFrame(prediction_electra[0])\n",
    "    df_electra=df_electra.rename(columns=dict(zip(df_electra.columns,['electra_'+str(col) for col in df_electra.columns])))\n",
    "    print(df_electra.head(),'\\n')\n",
    "    \n",
    "    \n",
    "    #XLNet\n",
    "    df_xlnet = pd.DataFrame(prediction_xlnet[0])\n",
    "    df_xlnet=df_xlnet.rename(columns=dict(zip(df_xlnet.columns,['xlnet_'+str(col) for col in df_xlnet.columns])))\n",
    "    print(df_xlnet.head(),'\\n')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #Combine the dataframes\n",
    "    df_combine = pd.concat([df_deberta, df_electra, df_xlnet], axis=1)\n",
    "    df_combine.head()\n",
    "    \n",
    "    \n",
    "    ###############################################\n",
    "    \n",
    "    #         ENSEMBLE SECTION\n",
    "    \n",
    "    ###############################################\n",
    "    \n",
    "    \n",
    "    # Importing the required packages\n",
    "    \n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import matthews_corrcoef\n",
    "    from sklearn.metrics import classification_report\n",
    "    from scipy.stats import pearsonr\n",
    "    from scipy.stats import spearmanr\n",
    "\n",
    "    import joblib\n",
    "\n",
    "    \n",
    "    # Split the dataset into train and test\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_combine, val_labels, test_size=0.3, random_state=100)\n",
    "    X_train.head()\n",
    "    \n",
    "    \n",
    "    ####\n",
    "    \n",
    "    # If task is stsb, variables are continuous, need a regressor enseble.\n",
    "    \n",
    "    if task != 'stsb':\n",
    "        model = GradientBoostingClassifier()\n",
    "        distributions = {'loss':['deviance', 'exponential'],\n",
    "                        \"n_estimators\":[100,200,500],\n",
    "                        'max_features':['sqrt', 'log2']}\n",
    "        clf = GridSearchCV(model, distributions, cv=5)\n",
    "        search = clf.fit(X_train, y_train)\n",
    "\n",
    "    else:\n",
    "        clf = GradientBoostingRegressor()\n",
    "        #model = RandomForestRegressor()\n",
    "        search = clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the values\n",
    "    y_pred = search.predict(X_test)\n",
    "    \n",
    "\n",
    "    ###\n",
    "    #Save the Random Forest\n",
    "    joblib.dump(search, \"./Ensemble Models/\"+task+\"_GradBoost_ensemble2.joblib\")\n",
    "\n",
    "    \n",
    "    # Print basic Report, then specify for the model\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"TASK : \", task)\n",
    "    print(\"Results Using All Features: \\n\")\n",
    "    \n",
    "    #Check Accuracy\n",
    "    if metric_name == 'accuracy':\n",
    "        ensemble_score = accuracy_score(y_test, y_pred)\n",
    "        try:\n",
    "            ensemble_f = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Extract the f-scores from the models\n",
    "            deberta_f = prediction_deberta.metrics['test_f1'], \n",
    "            electra_f = prediction_electra.metrics['test_f1'], \n",
    "            xlnet_f = prediction_xlnet.metrics['test_f1']\n",
    "        except:\n",
    "            ensemble_f = deberta_f = electra_f = xlnet_f = 999\n",
    "\n",
    "    elif metric_name == 'matthews_correlation':\n",
    "        ensemble_score = matthews_corrcoef(y_test, y_pred)\n",
    "        ensemble_f = deberta_f = electra_f = xlnet_f = 999\n",
    "\n",
    "    elif metric_name == \"pearson\":\n",
    "        ensemble_score = pearsonr(y_test, y_pred)[0]\n",
    "        ensemble_f = spearmanr(y_test, y_pred)[0]\n",
    "\n",
    "        # Extract the spearman scores from the models\n",
    "        deberta_f = prediction_deberta.metrics['test_spearmanr']\n",
    "        electra_f = prediction_electra.metrics['test_spearmanr'] \n",
    "        xlnet_f = prediction_xlnet.metrics['test_spearmanr']\n",
    "\n",
    "    else:\n",
    "        ensemble_score = 999\n",
    "        ensemble_f = 999\n",
    "        print('ERROR')\n",
    "\n",
    "    print(\"Accuracy : \", ensemble_score * 100, '\\nFscore : ', ensemble_f)\n",
    "        \n",
    "    print('-------------------')\n",
    "    print(\"DeBERTa : \", prediction_deberta.metrics['test_'+metric_name]*100)\n",
    "    print(\"Electra : \", prediction_electra.metrics['test_'+metric_name]*100)\n",
    "    print(\"XLNet : \", prediction_xlnet.metrics['test_'+metric_name]*100)\n",
    "    \n",
    "    \n",
    "    metric_collector.append([task,\n",
    "                             ensemble_score,\n",
    "                             prediction_deberta.metrics['test_'+metric_name], \n",
    "                             prediction_electra.metrics['test_'+metric_name], \n",
    "                             prediction_xlnet.metrics['test_'+metric_name],\n",
    "                             ensemble_f, deberta_f, electra_f, xlnet_f])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7605872",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9073e8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Ensemble</th>\n",
       "      <th>DeBERTa</th>\n",
       "      <th>Electra</th>\n",
       "      <th>XLNet</th>\n",
       "      <th>Ensemble_f</th>\n",
       "      <th>DeBERTa_f</th>\n",
       "      <th>Electra_f</th>\n",
       "      <th>XLNet_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stsb</td>\n",
       "      <td>0.894123</td>\n",
       "      <td>0.86789</td>\n",
       "      <td>0.872717</td>\n",
       "      <td>0.892639</td>\n",
       "      <td>0.888625</td>\n",
       "      <td>0.867806</td>\n",
       "      <td>0.871995</td>\n",
       "      <td>0.88876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Task  Ensemble  DeBERTa   Electra     XLNet  Ensemble_f  DeBERTa_f  \\\n",
       "0  stsb  0.894123  0.86789  0.872717  0.892639    0.888625   0.867806   \n",
       "\n",
       "   Electra_f  XLNet_f  \n",
       "0   0.871995  0.88876  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_metrics = pd.DataFrame(metric_collector, columns = ['Task','Ensemble', 'DeBERTa', 'Electra', 'XLNet', \n",
    "                                                             'Ensemble_f','DeBERTa_f', 'Electra_f', 'XLNet_f', ])\n",
    "ensemble_metrics.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1888ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove tuples from the f-scores in some columns\n",
    "\n",
    "for column in ['DeBERTa_f', 'Electra_f']:\n",
    "    col = ensemble_metrics[column]\n",
    "\n",
    "    for val in range(col.shape[0]):\n",
    "        #Correct tuples\n",
    "        if type(col[val]) == tuple:\n",
    "            ensemble_metrics[column][val] = col[val][0]\n",
    "            \n",
    "ensemble_metrics.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67391e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_metrics.to_csv('GradBoostEnsemble_save2.csv')\n",
    "ensemble_metrics.to_excel('GradBoostEnsemble_save2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "373490f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25375e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
